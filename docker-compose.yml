
services:
  embedding-model:
    image: ollama/ollama:latest
    hostname: embedding-model
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           capabilities: [gpu]
    #           count: all
    networks:
      - app-network
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    hostname: qdrant
    ports:
      - "6333:6333" 
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage 
    environment:
      - QDRANT__VECTOR_SIZE=512
      - QDRANT__HUSH_MODE=true 
    restart: unless-stopped 
    networks:
      - app-network
  
  ai-agent-api:
    hostname: ai-agent-api
    build:
      context: ./AiAgent.API/.
      dockerfile: Dockerfile
      args:
        BUILD_CONFIGURATION: Development   
    env_file:
      - .env
    environment:
      - OLLAMA_MODEL=${OLLAMA_MODEL}
      - OLLAMA_URI=${OLLAMA_URI}
      - QDRANT_HOSTAME=${QDRANT_HOSTNAME}
    ports:
      - "8080:8080"
      - "8081:8081"
    depends_on:
      - qdrant
      - embedding-model
    restart: unless-stopped
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  qdrant_storage:
  ollama_data:
